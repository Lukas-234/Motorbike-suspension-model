import numpy as np
from scipy.linalg import eigh
import matplotlib.pyplot as plt
import pandas as pd
from scipy import interpolate
from scipy.integrate import solve_ivp
from scipy.signal import welch

def k_finder(f_target, m_s, m_u, m_d, k_t):
    m_total=m_u+m_s
    k_estimate = (2 * np.pi * f_target)**2 * m_total
    k_s_lower = 0.5 * k_estimate
    k_s_higher = 2 * k_estimate
    def rf_func(k_s):
        """
        Returns: actual_frequency - target_frequency
        Root when this equals zero
        """
        K = np.array([[k_s, -k_s], 
                      [-k_s, k_s + k_t]])
        M = np.array([[m_s + m_d, 0], 
                      [0, m_u]])
        
        eigenvalues, eigenvectors = eigh(K, M)
        omega_squared = eigenvalues
        omega = np.sqrt(omega_squared)
        freq = omega / (2 * np.pi)
        participations = np.abs(eigenvectors[0,:])  # sprung mass DOF is index 0
        mode_index = np.argmax(participations)      # mode with largest sprung participation
        f_mode = freq[mode_index] # Sprung mass mode (lower frequency)
        
        return f_mode - f_target

    # Secant method implementation
    k_s_last, k_s_curr = k_s_lower, k_s_higher
    F_last, F_curr = rf_func(k_s_last), rf_func(k_s_curr)

    max_iterations = 50
    tolerance = 1e-4  # Hz

 #   print(f"Target frequency: {f_target} Hz Hz\n")

    for i in range(max_iterations):
        # Check convergence
        if abs(F_curr) < tolerance:
          #  print(f"found k in {i+1} iterations")
            break
        
        # Secant method update
        safe_gd = F_curr - F_last
        if abs(safe_gd) < 1e-12:
            safe_gd = np.sign(safe_gd) * 1e-12
        
        k_s_new = k_s_curr - F_curr * (k_s_curr - k_s_last) / safe_gd
        
        # Ensure positive stiffness
        if k_s_new <= 0:
            k_s_new = 1e-3
        
        # Update for next iteration
        k_s_last, F_last = k_s_curr, F_curr
        k_s_curr, F_curr = k_s_new, rf_func(k_s_new)
        
    else:
        print('✗ NO CONVERGENCE')
    per_spring=k_s_curr*0.5
    k_s_opt=k_s_curr
   # print(f"k_s_opt={k_s_opt:.2f} N/m")
    return k_s_opt
def c_finder(k_opt, delta_target, M, k_t):
    def rf_func2(k_s, c_s):
        K = np.array([[k_s, -k_s],
                      [-k_s, k_s + k_t]])
        C = np.array([[c_s, -c_s],
                      [-c_s, c_s]])
        M_inv = np.linalg.inv(M)
        
        A = np.block([[np.zeros((2,2)), np.eye(2)],
                      [-M_inv @ K, -M_inv @ C]])
        eigvals, eigvecs = np.linalg.eig(A)


    # Only positive imaginary part (oscillatory)
        pos_idx = [i for i, lam in enumerate(eigvals) if np.imag(lam) > 1e-8]
        eigvals = eigvals[pos_idx]
        eigvecs = eigvecs[:, pos_idx]

    # Pick mode with largest sprung-mass participation
        participations = [abs(eigvecs[0,i]) for i in range(len(eigvals))]
        mode_index = np.argmax(participations)
        lam_mode = eigvals[mode_index]

        sigma = np.real(lam_mode)
        omega_d = np.imag(lam_mode)
        delta = -sigma / np.sqrt(sigma**2 + omega_d**2)
        return delta
    def objective_func(c_s):
        return rf_func2(k_s_opt, c_s)- delta_target
        

    c_lower, c_upper = 500, 1500


    c_last, c_curr = c_lower, c_upper
    F_last, F_curr = objective_func(c_last), objective_func(c_curr)

    max_step=500
    for i in range(50):
        if abs(F_curr) < 1e-4:  # Tolerance on damping ratio
           # print(f"✓ found c in {i+1} iterations")
            break
        
        safe_gd = F_curr - F_last
        if abs(safe_gd) < 1e-12:
            safe_gd = np.sign(safe_gd) * 1e-12
        
        c_new = c_curr - F_curr * (c_curr - c_last) / safe_gd
        step = c_new - c_curr
        if abs(step) > max_step:
            step = np.sign(step) * max_step
        c_new = c_curr + step
        
       # Prevent unphysical or NaN values
        if np.isnan(c_new) or np.isinf(c_new):
            print("Warning: secant produced invalid c_s, stopping iteration")
            break
        
        
        c_last, F_last = c_curr, F_curr
        c_curr, F_curr = c_new, objective_func(c_new)

    c_s_opt = c_curr
    c_s_per_damper = c_s_opt / 2  # Twin shocks
 #   print(f"c_s_opt={c_s_opt:.2f} N/m")
    return c_s_opt

def road_analysis(k_s_opt, c_s_opt, roads, m_s, m_u, k_t):
    results=[]
    for road in roads:
        # === Cobbled Road Royal Mile ===
        df = pd.read_excel(road["file"])

        # velocity in m/s
        velocity = road["velocity_mph"] * 0.44704  

        # Extract data
        Latitude = df.iloc[:, 1].values
        Longitude = df.iloc[:, 2].values
        Altitude = df.iloc[:, 3].values

        # Convert degrees to meters
        conversion = 111139
        Latitude = np.array(Latitude) * conversion
        Longitude = np.array(Longitude) * conversion

        # Compute squared differences correctly
        Lat_diffs = np.diff(Latitude)
        Long_diffs = np.diff(Longitude)
        D2 = Lat_diffs**2 + Long_diffs**2
        D = np.sqrt(D2)

        # Cumulative distance (m)
        Dtotal = np.concatenate(([0.0], np.cumsum(D)))

        # Time = distance / velocity
        Ttotal = Dtotal / velocity

        # Make x-values for smooth interpolation
        x_lin = np.linspace(Ttotal[0], Ttotal[-1], 700)

        # Interpolation (set small smoothing for realistic curve)
        f_lin = interpolate.splrep(Ttotal, Altitude, s=0.05)
        Altitude_smooth = interpolate.splev(x_lin, f_lin, der=0)

        # Normalize road profile so it starts at 0 m
        Altitude_smooth -= Altitude_smooth[0]

        x_r_func = interpolate.interp1d(x_lin, Altitude_smooth, fill_value="extrapolate")
        x_r_dot_func = interpolate.splev(x_lin, f_lin, der=1)

        def x_r(t):
            return x_r_func(t)

        def x_r_dot(t):
            return float(np.interp(t, x_lin, x_r_dot_func))

        t_span = (0, x_lin.max())
        t_space = np.linspace(t_span[0], t_span[1], 1000)

        #c0 = 0.35*(2*(m_s*k_s)**0.5) #Ns/m
        alpha = 0.1 #10% variation due to environmental changes
        f = 1 #Hz frequency of variation of damping, allows for change every second
        
        def c(t):
            return c_s_opt * (1 + alpha * np.sin(2 * np.pi * f * t))

        def eom(t, y):
            x_s, x_s_dot, x_u, x_u_dot = y 
            xr = x_r(t)
            c_t = c(t)
            x_s_ddot = (1/(m_s)) * (-c_t * (x_s_dot - x_u_dot) - k_s_opt * (x_s - x_u))
            x_u_ddot = (1/m_u) * (c_t * (x_s_dot - x_u_dot) + k_s_opt * (x_s - x_u) - k_t * (x_u - xr))
            return [x_s_dot, x_s_ddot, x_u_dot, x_u_ddot]

        #Intial Condtitions to be changed [x_s, x_s_dot, x_u, x_u_dot]
        y0 = [0, 0, 0, 0]

        sol = solve_ivp(eom, t_span, y0, method='RK45', t_eval=t_space)

        # Extract results
        x_s, x_s_dot, x_u, x_u_dot = sol.y
        x_rp = x_r(sol.t)

        # Compute accelerations directly from eom()
        # (This ensures they are derived from the same ODE, not rederived manually)
        accels = np.array([eom(t, y) for t, y in zip(sol.t, sol.y.T)])
        x_s_ddot = accels[:, 1]
        x_u_ddot = accels[:, 3]
    
        
        t_start = 2.5  # seconds

        # Find the index where sol.t >= t_start
        idx_start = np.searchsorted(sol.t, t_start)

        # Slice time and corresponding data
        t_plot = sol.t[idx_start:]
        x_s_plot = x_s[idx_start:]
        x_s_dot_plot=x_s_dot[idx_start:]
        x_s_ddot_plot = x_s_ddot[idx_start:]
        x_u_plot=x_u[idx_start:]
        x_u_dot_plot=x_u_dot[idx_start:]
        x_u_ddot_plot=x_u_ddot[idx_start:]
        x_rp_plot = x_rp[idx_start:]
        
        
        fs = 1/np.mean(np.diff(t_plot))
        "rms acceleration of sprung mass weighted with frequency of acclerations according to"
        "iso, if frequencey of sprung mass acceleration lies in sensitive band more weight is given"
        "to that acceleration value , giving a better evaluation of comfort"
        
        def iso_weighting(x_s_ddot, fs, f_intervals, iso_weights):
            f, psd= welch(x_s_ddot, fs=fs)
            df=f[1]-f[0]
            unweighted_rms=[]
            for fi in f_intervals:
                f1=fi/2**(1/6)
                f2=fi*(2**(1/6))
                idx = np.where((f >= f1) & (f <= f2))[0]
                rms = np.sqrt(np.sum(psd[idx]) * df)
                unweighted_rms.append(rms)
            unweighted_rms=np.array(unweighted_rms)
           
            W= iso_weights/1000
            weighted_overall_rms = np.sqrt(np.sum((W * unweighted_rms)**2))
           
            return weighted_overall_rms
        f_intervals=np.array([0.1, 0.125, 0.16, 0.2, 0.25,
        0.315, 0.4, 0.5, 0.63, 0.8, 1, 1.25, 1.6, 2, 2.5, 3.15, 4, 5, 6.3, 8, 10,
        12.5, 16, 20, 25, 31.5, 40, 50, 63, 80, 100, 125, 160, 200, 250, 315, 400])
       
        iso_weights=np.array([31.2, 48.6, 79.0,
        121, 182, 263, 352, 418, 459, 477, 482, 484, 494, 531, 631, 804, 967, 1039,
        1054, 1036, 988, 902, 768, 636, 513, 405, 314, 246, 186, 132, 80.7, 54.0,
        20.5, 15.2, 7.90, 3.98, 1.95])
       
        rms_iso=iso_weighting(x_s_ddot, fs, f_intervals,iso_weights)
        # Reference (target) values
        rms_ref = 0.8        # m/s² from ideal ISO weighted acceleration
        stroke_ref = 0.08    # m approximateideal RMS stroke
        unsprung_ref = 12.0  # m/s² approx ideal unsprung acceleration
        heave_ref = 0.04     # m approx ideal body heave
        
        stroke = x_s_plot - x_u_plot
        stroke_rms = np.sqrt(np.mean(stroke**2))
        stroke_max = np.max(np.abs(stroke))
        sprung_rms = np.sqrt(np.mean(x_s_ddot_plot**2))
        unsprung_rms = np.sqrt(np.mean(x_u_ddot_plot**2))
        heave = np.sqrt(np.mean((x_s_plot - np.mean(x_s_plot))**2))
        
        rms_score = np.exp(-((rms_iso / rms_ref) ** 2))
        stroke_score = np.exp(-((stroke_rms / stroke_ref) ** 2))
        unsprung_score = np.exp(-((unsprung_rms / unsprung_ref) ** 2))
        heave_score = np.exp(-((heave / heave_ref) ** 2))
        comfort_metric = (
            0.5 * rms_score +     # main comfort driver
            0.2 * stroke_score +  # smooth motion
            0.2 * unsprung_score +# tire contact/harshness
            0.1 * heave_score     # low-frequency ride
        )
        results.append({
            "road": road["name"],
            "velocity_mph": road["velocity_mph"],
            "weighted_rms": rms_iso,
            "stroke_rms": stroke_rms ,
            "stroke_max":stroke_max,
            "comfort_metric":comfort_metric
        })

    # Return list of results for all roads
    return results

    
f_targets = np.array([1.0, 1.1, 1.2,1.3,1.4, 1.5])        # Hz
delta_targets = np.linspace(0.2, 0.35, 5)     # damping ratios
results=[]
for f_t in f_targets:
    for d_t in delta_targets:
        print(f"\nRunning for f_target={f_t:.2f} Hz, ζ_target={d_t:.2f}")
        m_s = 63 #kg mass acting on the rear of the bike (61.5%)
        m_u = 15 #kg mass of rear wheel and tyre
        m_d = 80 #kg mass of the driver to work out inertial force 
        k_t = 100000 #N/m rear tyre stiffness
        M = np.array([[m_s + m_d, 0], 
                      [0, m_u]])


        k_s_opt = k_finder(f_t, m_s, m_u, m_d, k_t)
        c_s_opt = c_finder(k_s_opt, d_t, M, k_t)
        
        roads = [
            {"file": "Cobbled Road Royal Mile.xlsx", "name": "Cobbled Road Royal Mile", "velocity_mph": 20},
            {"file": "Liberton Road (A Road).xlsx", "name": "Liberton Road", "velocity_mph": 30},
            {"file": "Pleasance Road (hilly).xlsx", "name": "Pleasance Road", "velocity_mph": 20},
            {"file": "Redford Road (B Road).xlsx", "name": "Redford Road", "velocity_mph": 30}
        ]
        
        # running road simulations to evaluate weighted rms accleration
        road_metrics = road_analysis(k_s_opt, c_s_opt, roads, m_s, m_u, k_t)
        avg_comfort =np.mean ([r["comfort_metric"] for r in road_metrics])
        
        results.append({
            "f_target": f_t,
            "zeta_target": d_t,
            "k_s_opt": k_s_opt,
            "c_s_opt": c_s_opt,
            "mean_comfort_metric": avg_comfort
        })
        
df_results = pd.DataFrame(results)

heatmap_data = df_results.pivot(index="zeta_target", columns="f_target", values="mean_comfort_metric")

plt.figure(figsize=(8,6))
# Use imshow
plt.imshow(heatmap_data, origin='lower', aspect='auto', cmap='viridis')
plt.colorbar(label='Mean Comfort Metric ')
plt.xticks(ticks=np.arange(len(heatmap_data.columns)), labels=np.round(heatmap_data.columns, 2))
plt.yticks(ticks=np.arange(len(heatmap_data.index)), labels=np.round(heatmap_data.index, 2))
plt.xlabel("Target Frequency f [Hz]")
plt.ylabel("Target Damping ζ")
plt.title("Design Sweep Heatmap:Comfort Metric")
plt.show()

# Find design with lowest mean weighted RMS
best_idx = df_results["mean_comfort_metric"].idxmin()
best_design = df_results.loc[best_idx]

print("BEST SUSPENSION DESIGN FOUND")
print(f"Target Frequency (f_target): {best_design['f_target']:.2f} Hz")
print(f"Target Damping Ratio (ζ_target): {best_design['zeta_target']:.2f}")
print(f"Optimal Spring Stiffness (k_s_opt): {best_design['k_s_opt']:.2f} N/m")
print(f"Optimal Damping Coefficient (c_s_opt): {best_design['c_s_opt']:.2f} N·s/m")
print(f"Comfort metric: {best_design['mean_comfort_metric']:.5f} m/s²")
print("="*70)


print("\nAll designs sorted by comfort (ascending comfort metric):")
print(df_results.sort_values("mean_comfort_metric").reset_index(drop=True))   
